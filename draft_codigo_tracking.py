# -*- coding: utf-8 -*-
"""draft_codigo_tracking.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TlrJzqnL3D3YrL8nAaRiWRxMygtGz-Fu
"""

import numpy as np
from scipy import stats,fftpack
import matplotlib.pyplot as plt
from keras.utils import to_categorical
#Imports NN
import numpy as np
from keras.models import Model
from keras.layers import Dense,BatchNormalization,Conv1D
from keras.layers import Input,GlobalMaxPooling1D,concatenate,Activation, Add
from keras.optimizers import Adam, SGD
from keras.layers import Dropout
from keras.callbacks import EarlyStopping,ReduceLROnPlateau,ModelCheckpoint

def two_state_switching_diffusion(n, k_state0, k_state1, D_state0, D_state1, T):
    x = np.random.normal(loc=0, scale=1, size=n)
    y = np.random.normal(loc=0, scale=1, size=n)
    
    #Residence time
    res_time0 = 1 / k_state0
    res_time1 = 1 / k_state1

    #Compute each t_state acording to exponential laws 
    t_state0 = np.random.exponential(scale=res_time0, size=n)
    t_state1 = np.random.exponential(scale=res_time1, size=n)

    #Set initial t_state for each state
    t_state0_next = 0
    t_state1_next = 0

    #Pick an initial state from a random choice
    current_state = np.random.choice([0, 1]) 
    
    #Detect real switching behavior 
    switching = ((current_state == 0) and (int(np.ceil(t_state0[t_state0_next])) < n)) or ((current_state == 1) and (int(np.ceil(t_state1[t_state1_next])) < n))

    #Fill state array 
    state = np.zeros(shape=n)
    i = 0

    while i < n:
        if current_state == 1:
            current_state_length = int(np.ceil(t_state1[t_state1_next]))
            
            if (current_state_length + i) < n:
                state[i:(i + current_state_length)] = np.ones(shape=current_state_length)
            else:
                state[i:n] = np.ones(shape=(n-i))
            
            current_state = 0 #Set state from 1->0
        else:
            current_state_length = int(np.ceil(t_state0[t_state0_next]))
            current_state = 1 #Set state from 0->1

        i += current_state_length
    
    for i in range(len(state)):
        if state[i] == 0:
            x[i] = x[i] * np.sqrt(2 * D_state0)
            y[i] = y[i] * np.sqrt(2 * D_state0)
        else:
            x[i] = x[i] * np.sqrt(2 * D_state1)
            y[i] = y[i] * np.sqrt(2 * D_state1)
    x = np.cumsum(x)
    y = np.cumsum(y)
    t = np.arange(0,n,1)/n
    t = t*T 
    return x,y,t,state,switching

def fbm_diffusion(n=1000,H=1,T=15):

    # first row of circulant matrix
    r = np.zeros(n+1)
    r[0] = 1
    idx = np.arange(1,n+1,1)
    r[idx] = 0.5*((idx+1)**(2*H) - 2*idx**(2*H) + (idx-1)**(2*H))
    r = np.concatenate((r,r[np.arange(len(r)-2,0,-1)]))
    
    # get eigenvalues through fourier transform
    lamda = np.real(fftpack.fft(r))/(2*n)
    
    # get trajectory using fft: dimensions assumed uncoupled
    x = fftpack.fft(np.sqrt(lamda)*(np.random.normal(size=(2*n)) + 1j*np.random.normal(size=(2*n))))
    x = n**(-H)*np.cumsum(np.real(x[:n])) # rescale
    x = ((T**H)*x)# resulting traj. in x
    y = fftpack.fft(np.sqrt(lamda)*(np.random.normal(size=(2*n)) + 1j*np.random.normal(size=(2*n))))
    y = n**(-H)*np.cumsum(np.real(y[:n])) # rescale
    y = ((T**H)*y) # resulting traj. in y

    t = np.arange(0,n+1,1)/n
    t = t*T # scale for final time T
    

    return x,y,t

# Generate mittag-leffler random numbers
def mittag_leffler_rand(beta = 0.5, n = 1000, gamma = 1):
    t = -np.log(np.random.uniform(size=[n,1]))
    u = np.random.uniform(size=[n,1])
    w = np.sin(beta*np.pi)/np.tan(beta*np.pi*u)-np.cos(beta*np.pi)
    t = t*((w**1/(beta)))
    t = gamma*t
    
    return t

# Generate symmetric alpha-levi random numbers
def symmetric_alpha_levy(alpha = 0.5,n=1000,gamma = 1):
    u = np.random.uniform(size=[n,1])
    v = np.random.uniform(size=[n,1])
    
    phi = np.pi*(v-0.5)
    w = np.sin(alpha*phi)/np.cos(phi)
    z = -1*np.log(u)*np.cos(phi)
    z = z/np.cos((1-alpha)*phi)
    x = gamma*w*z**(1-(1/alpha))
    
    return x

# needed for CTRW
def find_nearest(array, value):
    array = np.asarray(array)
    idx = (np.abs(array - value)).argmin()
    return idx

# Generate CTRW diffusion trajectory
def CTRW(n=1000,alpha=1,gamma=1,T=40):
    jumpsX = mittag_leffler_rand(alpha,n,gamma)

    rawTimeX = np.cumsum(jumpsX)
    tX = rawTimeX*(T)/np.max(rawTimeX)
    tX = np.reshape(tX,[len(tX),1])
    
    jumpsY = mittag_leffler_rand(alpha,n,gamma)
    rawTimeY = np.cumsum(jumpsY)
    tY = rawTimeY*(T)/np.max(rawTimeY)
    tY = np.reshape(tY,[len(tY),1])
    
    x = symmetric_alpha_levy(alpha=2,n=n,gamma=gamma**(alpha/2))
    x = np.cumsum(x)
    x = np.reshape(x,[len(x),1])
    
    y = symmetric_alpha_levy(alpha=2,n=n,gamma=gamma**(alpha/2))
    y = np.cumsum(y)
    y = np.reshape(y,[len(y),1])
    
    tOut = np.arange(0,n,1)*T/n
    xOut = np.zeros([n,1])
    yOut = np.zeros([n,1])
    for i in range(n):
        xOut[i,0] = x[find_nearest(tX,tOut[i]),0]
        yOut[i,0] = y[find_nearest(tY,tOut[i]),0]
    
    return xOut,yOut,tOut


def Sub_brownian(x0, n, dt, delta, out=None):
    x0 = np.asarray(x0)
    # generate a sample of n numbers from a normal distribution.
   
    r = stats.norm.rvs(size=x0.shape + (n,), scale=delta*np.sqrt(dt))

    # If `out` was not given, create an output array.
    if out is None:
        out = np.empty(r.shape)
    # Compute Brownian motion by forming the cumulative sum of random samples. 
    np.cumsum(r, axis=-1, out=out)
    # Add the initial condition.
    out += np.expand_dims(x0, axis=-1)

    return out

def Brownian(N=1000,T=50,delta=1):
    x = np.empty((2,N+1))
    x[:, 0] = 0.0
    
    Sub_brownian(x[:,0], N, T/N, delta, out=x[:,1:])
    
    out1 = x[0]
    out2 = x[1]
    
    return out1,out2

def generate_gaussian_noise(steps,sigma):
    noise_sample = np.sqrt(sigma)*np.random.randn(1,steps)
    return noise_sample

def generate(batchsize=32,steps=1000,T=15,sigma=0.1):
    while True:
        #Select randomly a type of motion
        out = np.zeros([batchsize,steps-1,1])
        label = np.zeros([batchsize,1])
        T_sample = np.random.choice(np.arange(T-5,T,0.5))
        steps_sample = int(np.random.choice(np.arange(steps+1, np.ceil(steps*1.2),1)))

        for i in range(batchsize):
            simulation_type = np.random.choice(["fbm","CTRW","Brownian"])

            if simulation_type == "fbm":
                fbm_type = np.random.choice(["super-diff", "sub-diff"])
                
                if fbm_type == "super-diff":
                    H = np.random.uniform(low=0.1, high=0.42)
                    label[i,0] = 0
                else: 
                    H = np.random.uniform(low=0.58, high=0.9)
                    label[i,0] = 0
                x,y,t = fbm_diffusion(steps_sample, H, T_sample)
            elif simulation_type == "Brownian":
                label[i,0] = 1
                x,y = Brownian(N=steps_sample,T=T_sample,delta=1)
            else:
                label[i,0] = 2
                alpha_sample = np.random.uniform(low=0.05,high=0.9)
                x,y,t = CTRW(n=steps_sample, alpha=alpha_sample, gamma=1,T=T_sample)
            
            """else: 
                # State0 Normal Diffusion 
                # State1 Confined Diffusion

                label[i,0] = 3
                switching = False
                while not switching:
                    k_state0 = np.random.uniform(low=0.01 ,high=0.08) 
                    k_state1 = np.random.uniform(low=0.007 ,high=0.2)
                    D_state0 = np.random.uniform(low=0.01 ,high=0.08) 
                    D_state1 = np.random.uniform(low=0.001 , high=0.01)
                    x,y,t,state,switching = two_state_switching_diffusion(steps_sample, k_state0, k_state1, D_state0, D_state1, T)"""

            # Only 1 dimension is being used because the simulations are homogeneous
            x_noise = generate_gaussian_noise(steps, sigma)
            x1 = np.reshape(x,[1,len(x)])
            x1 = x1 - np.mean(x1)
            x_n = x1[0,:steps] + x_noise
            dx = np.diff(x_n)
            out[i,:,0] = dx
            
    
        label = to_categorical(label,num_classes=3)
        yield out, label

def train(batchsize=32, steps = 100, T=15,sigma=0.1):
    initializer = 'he_normal'
    f = 32

    inputs = Input((steps-1,1))

    x1 = Conv1D(f,4,padding='causal',activation='relu',kernel_initializer=initializer)(inputs)
    x1 = BatchNormalization()(x1)
    x1 = Conv1D(f,4,dilation_rate=2,padding='causal',activation='relu',kernel_initializer=initializer)(x1)
    x1 = BatchNormalization()(x1)
    x1 = Conv1D(f,4,dilation_rate=4,padding='causal',activation='relu',kernel_initializer=initializer)(x1)
    x1 = BatchNormalization()(x1)
    x1 = GlobalMaxPooling1D()(x1)


    x2 = Conv1D(f,2,padding='causal',activation='relu',kernel_initializer=initializer)(inputs)
    x2 = BatchNormalization()(x2)
    x2 = Conv1D(f,2,dilation_rate=2,padding='causal',activation='relu',kernel_initializer=initializer)(x2)
    x2 = BatchNormalization()(x2)
    x2 = Conv1D(f,2,dilation_rate=4,padding='causal',activation='relu',kernel_initializer=initializer)(x2)
    x2 = BatchNormalization()(x2)
    x2 = GlobalMaxPooling1D()(x2)


    x3 = Conv1D(f,3,padding='causal',activation='relu',kernel_initializer=initializer)(inputs)
    x3 = BatchNormalization()(x3)
    x3 = Conv1D(f,3,dilation_rate=2,padding='causal',activation='relu',kernel_initializer=initializer)(x3)
    x3 = BatchNormalization()(x3)
    x3 = Conv1D(f,3,dilation_rate=4,padding='causal',activation='relu',kernel_initializer=initializer)(x3)
    x3 = BatchNormalization()(x3)
    x3 = GlobalMaxPooling1D()(x3)


    x4 = Conv1D(f,10,padding='causal',activation='relu',kernel_initializer=initializer)(inputs)
    x4 = BatchNormalization()(x4)
    x4 = Conv1D(f,10,dilation_rate=4,padding='causal',activation='relu',kernel_initializer=initializer)(x4)
    x4 = BatchNormalization()(x4)
    x4 = Conv1D(f,10,dilation_rate=8,padding='causal',activation='relu',kernel_initializer=initializer)(x4)
    x4 = BatchNormalization()(x4)
    x4 = GlobalMaxPooling1D()(x4)


    x5 = Conv1D(f,20,padding='same',activation='relu',kernel_initializer=initializer)(inputs)
    x5 = BatchNormalization()(x5)
    x5 = GlobalMaxPooling1D()(x5)


    con = concatenate([x1,x2,x3,x4,x5])
    dense = Dense(512,activation='relu')(con)
    dense = Dense(128,activation='relu')(dense)
    dense2 = Dense(3,activation='sigmoid')(dense)
    model = Model(inputs=inputs, outputs=dense2)

    optimizer = Adam(lr=1e-5)
    model.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['acc','mse'])
    model.summary()

    callbacks = [EarlyStopping(monitor='val_loss',
                        patience=20,
                        verbose=0,
                        min_delta=1e-4),
            ReduceLROnPlateau(monitor='val_loss',
                            factor=0.1,
                            patience=4,
                            verbose=0,
                            min_lr=1e-9)]

    model.fit_generator(
            generator=generate(batchsize=batchsize,steps=steps,T=T,sigma=sigma),
            steps_per_epoch=4000,
            epochs=50,
            callbacks=callbacks,
            validation_data=generate(batchsize=batchsize,steps=steps,T=T,sigma=sigma),
            validation_steps=10)
    return model



def accuracy_check(sample_size = 100, N=100,T=15):
    
    categories = 3

    for i in range(categories):
        fbm, brownian, ctrw = 0,0,0,0

        for j in range(sample_size):
            if i == 0:
                H = np.random.uniform(low=0.1, high=0.4)
                x,y,t = fbm_diffusion(N, H=H, T=T)
            elif i==1:
                H = np.random.uniform(low=0.6, high=0.9)
                x,y,t = fbm_diffusion(N, H=H, T=T)
            elif i==2:
                x,y = Brownian(N=N,T=T,delta=1)
            else:
                alpha_sample = np.random.uniform(low=0.05,high=0.9)
                x,y,t = CTRW(n=N, alpha=alpha_sample, gamma=1,T=T)
            
            input_net_x = np.zeros([1,N-1,1])
            input_net_y = np.zeros([1,N-1,1])
            x1 = np.reshape(x,[1,len(x)])
            y1 = np.reshape(y,[1,len(y)])
            x1 = x1-np.mean(x1)
            y1 = y1-np.mean(y1)
            dx = np.diff(x1)
            dy = np.diff(y1)    
            input_net_x[0,:,0] = dx[:,0:99]
            input_net_y[0,:,0] = dy[:,0:99]
            prediction_model = (model.predict(input_net_x) + model.predict(input_net_y)) / 2
            prediction = np.argmax(prediction_model)

            if prediction == 0:
                fbm += 1
            elif prediction == 1:
                brownian += 1
            else:
                ctrw += 1
            
        print("<-------------------------------------------------------------->")
        if i == 0:
            print("fbm simuls")
        elif i==1:
            print("Brownian simuls")
        else:
            print("ctrw simuls")
        print(f'Browian:{brownian/sample_size}\t fbm:{fbm/sample_size}\t CTRW:{ctrw/sample_size}')

model = train(batchsize = 64, steps=100, T=15, sigma=0.1)
accuracy_check(sample_size=1000,N=100,T=150)

